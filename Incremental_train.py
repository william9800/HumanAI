#å®Œæˆå¢é‡è®­ç»ƒä»»åŠ¡
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from efficientnet_pytorch import EfficientNet
from dataset.dataset import HumanAI
from tqdm import tqdm
from torch.utils.data import ConcatDataset

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ•°æ®å¢å¼º
transform = transforms.Compose([
    transforms.CenterCrop(600),  # EfficientNet-B7æ¨èå°ºå¯¸
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# æ¸…ç†æŸåå›¾
def collate_fn(batch):
    batch = [x for x in batch if x[0] is not None]
    return torch.utils.data.dataloader.default_collate(batch) if batch else None

# è®­ç»ƒå‡½æ•°
def train(model, loader, optimizer, criterion, epoch, scaler, use_amp=True):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    bar = tqdm(loader, desc=f"ğŸš€ Fine-tune Epoch {epoch}")

    for data, target in bar:
        data, target = data.to(DEVICE), target.to(DEVICE)
        optimizer.zero_grad()

        with torch.amp.autocast(device_type='cuda', enabled=use_amp):
            output = model(data)
            loss = criterion(output, target)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item() * data.size(0)
        pred = output.argmax(1)
        correct += (pred == target).sum().item()
        total += target.size(0)

        bar.set_postfix(loss=loss.item(), acc=correct / total * 100)

    avg_loss = total_loss / total
    acc = correct / total * 100
    print(f"âœ… Epoch {epoch} å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.4f}, å‡†ç¡®ç‡: {acc:.2f}%")

# éªŒè¯å‡½æ•°
def validate(model, loader, criterion):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(DEVICE), target.to(DEVICE)
            output = model(data)
            loss = criterion(output, target)
            total_loss += loss.item() * data.size(0)
            pred = output.argmax(1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    acc = correct / total * 100
    print(f"ğŸ¯ éªŒè¯é›†å‡†ç¡®ç‡: {acc:.2f}%ï¼Œå¹³å‡æŸå¤±: {total_loss / total:.4f}")
    return acc

# ä¸»å‡½æ•°ï¼šè°ƒç”¨å…¥å£
def finetune(base_model_path, old_data_dir, new_data_dir, val_data_dir=None,
             freeze_backbone=True, epochs=5, batch_size=32, save_path='model_finetuned.pth'):

    print(f"\nğŸ“¥ åŠ è½½æ¨¡å‹: {base_model_path}")
    model = EfficientNet.from_name('efficientnet-b7')
    model._fc = nn.Linear(model._fc.in_features, 2)
    model.load_state_dict(torch.load(base_model_path, map_location=DEVICE))
    model.to(DEVICE)

    if freeze_backbone:
        for param in model.parameters():
            param.requires_grad = False
        for param in model._fc.parameters():
            param.requires_grad = True
        print("ğŸ§Š å†»ç»“ä¸»å¹²ï¼Œä»…å¾®è°ƒå…¨è¿æ¥å±‚")

    # ğŸ” åŠ è½½æ—§æ•°æ® + æ–°æ•°æ®
    dataset_old = HumanAI(old_data_dir, transforms=transform, train=True)
    dataset_new = HumanAI(new_data_dir, transforms=transform, train=True)

    # âœ… åˆå¹¶ä¸ºä¸€ä¸ªè®­ç»ƒé›†
    combined_dataset = ConcatDataset([dataset_old, dataset_new])

    train_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)

    if val_data_dir:
        val_dataset = HumanAI(val_data_dir, transforms=transform, train=False)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)
    else:
        val_loader = None


    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)
    scaler = torch.amp.GradScaler()

    for epoch in range(1, epochs + 1):
        train(model, train_loader, optimizer, criterion, epoch, scaler)
        if val_loader:
            validate(model, val_loader, criterion)
        scheduler.step()

    torch.save(model.state_dict(), save_path)
    print(f"\nâœ… å¢é‡è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜ä¸º: {save_path}")

# ç›´æ¥è¿è¡Œå‘½ä»¤è¡Œ
if __name__ == '__main__':
    # ç¤ºä¾‹è°ƒç”¨ï¼ˆå¯ä¿®æ”¹ï¼‰
    finetune(
        base_model_path="model_stylegan2.pth",
        old_data_dir="data/train",
        new_data_dir="data/train_stablediffusion",
        val_data_dir="data/test",
        freeze_backbone=False,
        epochs=5,
        batch_size=32,
        save_path="model_stylegan2+sd_mixed.pth"
    )